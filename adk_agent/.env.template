# =============================================================================
# ADK AGENT SERVER - ENVIRONMENT CONFIGURATION
# =============================================================================
# Copy this file to .env and configure your environment variables
# Required variables are marked with [REQUIRED]


# =============================================================================
# ‚ö° QUICK START - MANDATORY CONFIGURATION
# =============================================================================

# [REQUIRED] Agent Model
# Available models: gemini-2.5-flash-lite, claude-4-sonnet, gpt-4, etc.
MODEL_ID=claude-4-sonnet

# [REQUIRED] Agent Identity
AGENT_NAME=adk_agent


# =============================================================================
# ü§ñ LLM PROVIDER CONFIGURATION (Choose ONE)
# =============================================================================
# You must configure EITHER direct API access OR LiteLLM proxy

# -----------------------------------------------------------------------------
# OPTION 1: Direct Google API Access
# -----------------------------------------------------------------------------
# [REQUIRED if not using LiteLLM] Google API Key for Gemini models
# Get your key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=

# -----------------------------------------------------------------------------
# OPTION 2: LiteLLM Proxy (Unified LLM Access)
# -----------------------------------------------------------------------------
# [REQUIRED if not using direct API] Use LiteLLM proxy for any LLM provider
# Set to "True" to enable LiteLLM proxy mode
USE_LITELLM_PROXY=False

# LiteLLM proxy endpoint (required if USE_LITELLM_PROXY=True)
# Use https://openrouter.ai/api/v1 for OpenRouter service
LITELLM_PROXY_API_BASE=http://localhost:4000

# LiteLLM proxy API key (required if USE_LITELLM_PROXY=True)
LITELLM_PROXY_API_KEY=

# OpenRouter API key (if using OpenRouter as LiteLLM backend)
OPENROUTER_API_KEY=


# =============================================================================
# üóÑÔ∏è  DATABASE CONFIGURATION
# =============================================================================

# Session storage database URL
# Default: In-memory SQLite (data lost on restart)
# For persistence, use file-based SQLite or PostgreSQL
#
# Examples:
#   In-memory SQLite:  sqlite:///:memory:
#   File SQLite:       sqlite:///./sessions.db
#   PostgreSQL:        postgresql+psycopg://user:password@localhost:5432/mydb
DATABASE_URL=sqlite:///:memory:


# =============================================================================
# üåê SERVER CONFIGURATION
# =============================================================================

# Server host and port
HOST=0.0.0.0
PORT=8000

# Enable web interface (provides a UI for interacting with agents)
SERVE_WEB_INTERFACE=true

# Enable A2a (Agent-to-Agent) protocol support
ENABLE_A2A=True


# =============================================================================
# üîê OPTIONAL: API AUTHENTICATION
# =============================================================================
# Protect your API endpoints with API keys
# If set, all endpoints (except /health, /docs) require X-API-KEY header

# Comma-separated list of valid API keys
# Example: ADK_API_KEYS=key1,key2,key3
ADK_API_KEYS=


# =============================================================================
# üìä OPTIONAL: OBSERVABILITY & MONITORING
# =============================================================================

# Enable Langfuse tracing and monitoring
# Provides detailed insights into agent interactions and performance
LANGFUSE_ENABLED=false

# Langfuse credentials (required if LANGFUSE_ENABLED=true)
# Get credentials from: https://cloud.langfuse.com
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=

# Langfuse server URL
LANGFUSE_HOST=http://localhost:4317

# Enable cloud tracing (for production deployments)
TRACE_TO_CLOUD=false

# Logging configuration
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s


# =============================================================================
# üîß ADVANCED: CUSTOM AGENT PROMPTS
# =============================================================================
# Override default agent instruction and description
# Leave empty to use defaults from prompts.py

MAIN_AGENT_INSTRUCTION=
MAIN_AGENT_DESCRIPTION=


# =============================================================================
# ü§ñ SUB-AGENTS CONFIGURATION
# =============================================================================

# -----------------------------------------------------------------------------
# Database Agent (db_agent) - MCP Toolbox Integration
# -----------------------------------------------------------------------------
# Configuration for the database sub-agent that handles data operations
# using the MCP (Model Context Protocol) toolbox

# MCP Toolbox server URL
# Default: http://localhost:9000
TOOLBOX_URL=http://localhost:9000

# Name of the toolset to load from the MCP Toolbox
# This should match the toolset name configured in your MCP server
TOOLBOX_TOOLSET=my-toolset

# Optional: Override model for database agent
# If not set, inherits from MODEL_ID (shared configuration)
# Example: DB_AGENT_MODEL_ID=gemini-2.5-flash-lite
DB_AGENT_MODEL_ID=

# Optional: Custom name for database agent instance
# Default: db_agent
DB_AGENT_NAME=db_agent

# Optional: Custom prompts for database agent
# Override the default database agent behavior and description
DB_AGENT_INSTRUCTION=
DB_AGENT_DESCRIPTION=


# =============================================================================
# üîß ADVANCED: ADDITIONAL SERVICES
# =============================================================================
# URIs for optional ADK services
# Leave empty if not using these services

# Artifact service for storing agent-generated files
ARTIFACT_SERVICE_URI=

# Memory service for long-term agent memory
MEMORY_SERVICE_URI=


# =============================================================================
# üìù CONFIGURATION GUIDE
# =============================================================================
#
# 1Ô∏è‚É£  MINIMAL SETUP (local development):
#    Choose ONE LLM provider option:
#
#    OPTION A - Google Gemini (Direct):
#      ‚Ä¢ Set GOOGLE_API_KEY
#      ‚Ä¢ Keep USE_LITELLM_PROXY=False
#
#    OPTION B - Any LLM via LiteLLM Proxy:
#      ‚Ä¢ Set USE_LITELLM_PROXY=True
#      ‚Ä¢ Set LITELLM_PROXY_API_BASE (your LiteLLM server)
#      ‚Ä¢ Set LITELLM_PROXY_API_KEY (your proxy key)
#
#    ‚Ä¢ Keep defaults for MODEL_ID, AGENT_NAME, DATABASE_URL
#    ‚Ä¢ Server will run on http://localhost:8000
#
# 2Ô∏è‚É£  RAG FUNCTIONALITY (Optional):
#    ‚Ä¢ Requires RAG service running separately
#    ‚Ä¢ Set RAG_SERVICE_URL, RAG_REDIS_URL, RAG_INDEX_NAME
#    ‚Ä¢ If not configured, agent will work without RAG search tools
#
# 3Ô∏è‚É£  PRODUCTION DEPLOYMENT:
#    ‚Ä¢ Set DATABASE_URL to PostgreSQL for persistence
#    ‚Ä¢ Enable API authentication with ADK_API_KEYS
#    ‚Ä¢ Consider enabling LANGFUSE_ENABLED for monitoring
#    ‚Ä¢ Set SERVE_WEB_INTERFACE=false if only using API
#
# 4Ô∏è‚É£  A2A PROTOCOL (Automatic):
#    ‚Ä¢ No configuration needed - automatically enabled!
#    ‚Ä¢ A2A endpoints created at: /a2a/{agent_name}/
#    ‚Ä¢ Add agent.json file in any subfolder to create new A2A agents
#    ‚Ä¢ Example: /a2a/app/ is created from app/agent.json
#
# 5Ô∏è‚É£  SUB-AGENTS (Optional):
#    ‚Ä¢ Database Agent (db_agent): Configure TOOLBOX_URL and TOOLBOX_TOOLSET
#    ‚Ä¢ MCP Server: Must be running at TOOLBOX_URL before using db_agent
#    ‚Ä¢ Model Inheritance: DB_AGENT_MODEL_ID inherits from MODEL_ID if not set
#    ‚Ä¢ Custom Behavior: Override with DB_AGENT_INSTRUCTION and DB_AGENT_DESCRIPTION
#    ‚Ä¢ Sub-agents share base configuration (model_id, agent_name) but can override
#
# 6Ô∏è‚É£  USEFUL ENDPOINTS:
#    ‚Ä¢ Swagger UI:         http://localhost:8000/docs
#    ‚Ä¢ ReDoc:              http://localhost:8000/redoc
#    ‚Ä¢ Health check:       http://localhost:8000/health
#    ‚Ä¢ A2A agent card:     http://localhost:8000/a2a/app/.well-known/agent-card.json
#    ‚Ä¢ A2A JSON-RPC:       http://localhost:8000/a2a/app (POST)
#
# 7Ô∏è‚É£  LITELLM PROXY BENEFITS:
#    ‚Ä¢ Unified interface for any LLM (OpenAI, Anthropic, Google, etc.)
#    ‚Ä¢ Load balancing across multiple models
#    ‚Ä¢ Cost tracking and rate limiting
#    ‚Ä¢ Fallback support between providers
#    ‚Ä¢ More info: https://docs.litellm.ai/docs/proxy/quick_start
#
